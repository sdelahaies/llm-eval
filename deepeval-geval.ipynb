{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepeval-geval\n",
    "\n",
    "code reproduced from https://www.datacamp.com/tutorial/deepeval\n",
    "\n",
    "Using ollama to serve phi4 locally\n",
    "\n",
    "This notebook demonstrates how to use DeepEval, a framework for evaluating large language models (LLMs), to assess the correctness of generated outputs. The focus is on utilizing the GEval metric to evaluate LLM responses based on factual accuracy, completeness, and alignment with expected outputs. By running through a few test cases, we can see how GEval provides structured, meaningful feedback for LLM evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üôå Congratulations! You're now using a local model for all evals that require an\n",
      "LLM.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-local-model --model-name=phi4 --base-url=\"http://localhost:11434/v1/\" --api-key=\"ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    #model=\"gpt-4o\", set local model instead\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should also lightly penalize omission of detail, and focus on the main idea\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "\n",
    "first_test_case = LLMTestCase(input=\"What are the main causes of deforestation?\",\n",
    "                              actual_output=\"The main causes of deforestation include agricultural expansion, logging, infrastructure development, and urbanization.\",\n",
    "                              expected_output=\"The main causes of deforestation include agricultural expansion, logging, infrastructure development, and urbanization.\")\n",
    "\n",
    "\n",
    "second_test_case = LLMTestCase(input=\"Define the term 'artificial intelligence'.\",\n",
    "                               actual_output=\"Artificial intelligence is the simulation of human intelligence by machines.\",\n",
    "                               expected_output=\"Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn like humans, including tasks such as problem-solving, decision-making, and language understanding.\")\n",
    "\n",
    "\n",
    "third_test_case = LLMTestCase(input=\"List the primary colors.\",\n",
    "                              actual_output=\"The primary colors are green, orange, and purple.\",\n",
    "                              expected_output=\"The primary colors are red, blue, and yellow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [first_test_case, second_test_case, third_test_case]\n",
    "\n",
    "dataset = EvaluationDataset(test_cases=test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using local model, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing local model, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 3 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (3/3) [Time Taken: 00:06,  2.23s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Correctness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: local model, reason: The actual output matches the expected output exactly, with no contradictions or omissions of detail. The main idea is fully captured without any vague language or contradicting opinions., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What are the main causes of deforestation?\n",
      "  - actual output: The main causes of deforestation include agricultural expansion, logging, infrastructure development, and urbanization.\n",
      "  - expected output: The main causes of deforestation include agricultural expansion, logging, infrastructure development, and urbanization.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Correctness (GEval) (score: 0.7, threshold: 0.5, strict: False, evaluation model: local model, reason: The actual output captures the main idea of simulating human intelligence by machines, aligning with the expected output. However, it omits details about programming machines to think and learn like humans, including problem-solving, decision-making, and language understanding., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Define the term 'artificial intelligence'.\n",
      "  - actual output: Artificial intelligence is the simulation of human intelligence by machines.\n",
      "  - expected output: Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn like humans, including tasks such as problem-solving, decision-making, and language understanding.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.2, threshold: 0.5, strict: False, evaluation model: local model, reason: The actual output contradicts the expected output by listing green, orange, and purple as primary colors instead of red, blue, and yellow. This is a significant factual error. Additionally, there is an omission of detail regarding the correct primary colors., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: List the primary colors.\n",
      "  - actual output: The primary colors are green, orange, and purple.\n",
      "  - expected output: The primary colors are red, blue, and yellow.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness (GEval): 66.67% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_output = dataset.evaluate([correctness_metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
